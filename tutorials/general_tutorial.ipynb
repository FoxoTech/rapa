{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAPA Walkthrough\n",
    "This tutorial is meant to cover multiple RAPA use cases, including starting from scratch or using a previous DataRobot project.\n",
    "\n",
    "### Overview\n",
    "1. Initialize the DataRobot API \n",
    "    - Save a pickled dictionary for DataRobot API Initialization\n",
    "    - Use the pickled dictionary to initialize the DataRobot API\n",
    "    - **(Optional)**: Skip this if the DataRobot API is previously initialized\n",
    "2. Submit data as a project to DataRobot\n",
    "    - Create a submittable `pandas` dataframe\n",
    "    - Submit the data using RAPA\n",
    "    - **(Optional)**: If parsimonious feature reduction is required on an existing project, it is possible to load the project instead of creating a new one.\n",
    "3. Perform parsimonious feature reduction\n",
    "\n",
    "<a href=\"https://life-epigenetics-rapa.readthedocs-hosted.com/en/latest/\">[Link to the documentation]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0.4\n"
     ]
    }
   ],
   "source": [
    "import rapa\n",
    "print(rapa.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize DataRobot the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the <a href=\"app.datarobot.com\"> DataRobot website </a>, find the developer tools and retrieve an API key. Once you have a key, make sure to run the next block of code with your api key replacing the value in the dictionary. Here is a <a href=\"https://docs.datarobot.com/en/docs/api/api-quickstart/api-qs.html\">detailed article on DataRobot API keys</a>.\n",
    "\n",
    "**Make sure to remove code creating the pickled dataframe and the pickled dataframe itself from any public documents, such as GitHub.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00-data-preparation.ipynb',\n",
       " '02-tutorial.ipynb',\n",
       " 'general_tutorial.ipynb',\n",
       " '01-demo-rapa.ipynb']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data folder in the current directory.\n"
     ]
    }
   ],
   "source": [
    "# save a pickled dictionary for datarobot api initialization\n",
    "api_dict = {'tutorial':'APIKEYHERE'}\n",
    "if 'data' in os.listdir('.'):\n",
    "    print('data folder already exists, skipping folder creation...')\n",
    "else:\n",
    "    print('Creating data folder in the current directory.')\n",
    "    os.mkdir('data')\n",
    "\n",
    "if 'dr-tokens.pkl' in os.listdir('data'):\n",
    "    print('dr-tokens.pkl already exists.')\n",
    "else:\n",
    "    with open('data/dr-tokens.pkl', 'wb') as handle:\n",
    "        pickle.dump(api_dict, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Unable to authenticate to the server - are you sure the provided token of \"APIKEYHERE\" and endpoint of \"https://app.datarobot.com/api/v2\" are correct? Note that if you access the DataRobot webapp at `https://app.datarobot.com`, then the correct endpoint to specify would be `https://app.datarobot.com/api/v2`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f1d11b507d25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Use the pickled dictionary to initialize the DataRobot API\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrapa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_dr_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tutorial'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/rapa/utils.py\u001b[0m in \u001b[0;36minitialize_dr_api\u001b[0;34m(token_key, file_path, endpoint)\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0mdr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatarobot_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# check to see if w is not None or empty (has a warning)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Unable to authenticate to the server - are you sure the provided token of \"APIKEYHERE\" and endpoint of \"https://app.datarobot.com/api/v2\" are correct? Note that if you access the DataRobot webapp at `https://app.datarobot.com`, then the correct endpoint to specify would be `https://app.datarobot.com/api/v2`."
     ]
    }
   ],
   "source": [
    "# Use the pickled dictionary to initialize the DataRobot API\n",
    "rapa.utils.initialize_dr_api('tutorial')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of this tutorial uses the DataRobot API, so if the API is not initialized, it will not run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Submit data as a project to DataRobot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial uses the Breast cancer wisconsin (diagnostic) dataset as an easily accessible example set for `rapa`, as it easily loaded with `sklearn`. \n",
    "\n",
    "This breast cancer dataset has **30 features** extracted from digitized images of aspirated breast mass cells. A few features are the mean radius of the cells, the mean texture, mean perimater  The **target** is whether the cells are from a malignant or benign tumor, with 1 indicating benign and 0 indicating malignant. There are 357 benign and 212 malignant samples, making 569 samples total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets # data used in this tutorial\n",
    "import pandas as pd # used for easy data management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads the dataset (as a dictionary)\n",
    "breast_cancer_dataset = datasets.load_breast_cancer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# puts features and targets from the dataset into a dataframe\n",
    "breast_cancer_df = pd.DataFrame(data=breast_cancer_dataset['data'], columns=breast_cancer_dataset['feature_names'])\n",
    "breast_cancer_df['benign'] = breast_cancer_dataset['target']\n",
    "print(breast_cancer_df.shape)\n",
    "breast_cancer_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using `rapa` to create a project on DataRobot, the number of features is reduced using on of the sklearn functions `sklearn.feature_selection.f_classif`, or `sklearn.feature_selection.f_regress` depending on the `rapa` instance that is called. In this tutorial's case, the data is a binary classification problem, so we have to create an instance of the `rapa.RAPAClassif` class.\n",
    "\n",
    "As of now, `rapa` only supports classification and regression problems on DataRobot. Additionally, `rapa` has only been tested on tabular data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a rapa classifcation object\n",
    "depression_classification = rapa.rapa.RAPAClassif()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a datarobot submittable dataframe with cross validation folds stratified for the target (benign)\n",
    "sub_df = depression_classification.create_submittable_dataframe(breast_cancer_df, target_name='benign')\n",
    "print(sub_df.shape)\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submits a project to datarobot using our dataframe, target, and project name.\n",
    "project = depression_classification.submit_datarobot_project(input_data_df=sub_df, target_name='benign', project_name='TUTORIAL_breast_cancer')\n",
    "project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the project already exists, the `rapa.utils.find_project` function can be used to search for a project\n",
    "project = rapa.utils.find_project(\"TUTORIAL_breast_cancer\")\n",
    "project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Perform parsimonious feature reduction\n",
    "\n",
    "`rapa`'s main function is `perform_parsimony`. Requiring a *feature_range* and a *project*, this function recursively removes features by their relative feature impact scores across all models in a featurelist, creating a new featurelist and set of models with DataRobot each iteration.\n",
    "\n",
    "* **feature_range:** a list of desired featurelist lengths as integers (Ex: [25, 20, 15, 10, 5, 4, 3, 2, 1]), or of desired featurelist sizes (Ex: [0.9, 0.7, 0.5, 0.3, 0.1]). This tells `rapa` how many features remain after each iteration of feature reduction.\n",
    "* **project:** either a `datarobot` project, or a string of it’s id or name. `rapa.utils.find_project` can be used to find a project already existing in DataRobot, or `submit_datarobot_project` can be used to submit a new project.\n",
    "* **featurelist_prefix:** provides `datarobot` with a prefix that will be used for all the featurelists created by the `perform_parsimony` function. If running `rapa` multiple times in one DataRobot project, make sure to change the **featurelist_prefix** each time to avoid confusion.\n",
    "* **starting_featurelist_name:** the name of the featurelist you would like to start parsimonious reduction from. It defaults to 'Informative Features', but can be changed to any featurelist name that exists within the project.\n",
    "* **lives:** number of times allowed for reducing the featurelist and obtaining a worse model. By default, ‘lives’ are off, and the entire ‘feature_range’ will be ran, but if supplied a number >= 0, then that is the number of ‘lives’ there are. (Ex: lives = 0, feature_range = [100, 90, 80, 50] RAPA finds that after making all the models for the length 80 featurelist, the ‘best’ model was created with the length 90 featurelist, so it stops and doesn’t make a featurelist of length 50.) This is similar to DataRobot’s <a href=\"https://www.datarobot.com/blog/using-feature-importance-rank-ensembling-fire-for-advanced-feature-selection/\">Feature Importance Rank Ensembling for advanced feature selection (FIRE)</a> package’s ‘lifes’.\n",
    "* **cv_average_mean_error_limit**: limit of cross validation mean error to help avoid overfitting. By default, the limit is off, and the each ‘feature_range’ will be ran. Limit exists only if supplied a number >= 0.0.\n",
    "* **to_graph**: a list of keys choosing which graphs to produce. Current graphs are *feature_performance* and *models*. *feature_performance* graphs a stackplot of feature impacts across many featurelists, showing the change in impact over different featurelist lengths. *models* plots `seaborn` boxplots of some metric of accuracy for each featurelist length. These plots are created after each iteration.\n",
    "\n",
    "Additional arguments and their effects can be found <a href=\"https://life-epigenetics-rapa.readthedocs-hosted.com/en/latest/\">in the API documentation</a>, or within the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = depression_classification.perform_parsimony(project=project, \n",
    "                                            featurelist_prefix='TEST_0.0', \n",
    "                                            starting_featurelist_name='Informative Features', \n",
    "                                            feature_range=[25, 20, 15, 10, 5, 4, 3, 2, 1],\n",
    "                                            lives=5,\n",
    "                                            cv_average_mean_error_limit=.8,\n",
    "                                            to_graph=['feature_performance', 'models'])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "15d1d4f8c2954fff1d3da98431dbef1005e87db545c641a7b60a75b16bf902c6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
